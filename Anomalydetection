from collections import OrderedDict

import torch
from torch.autograd import Variable
from torchvision import models
import sys
import numpy as np
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import argparse
from operator import itemgetter
import time
import tensorly as tl
import tensorly
from itertools import chain
import CNNmodel as CNN
import CNNCPU as  CPU


def k_largest_index_argsort(a, k):
    idx = np.argsort(a.ravel())[:-k - 1:-1]

    return np.column_stack(np.unravel_index(idx, a.shape))


def accuracy_detection(anomaly_data, detected_data):
    TP = 0
    FP = 0
    FN = 0
    TN = 0
    for i in range(anomaly_data.shape[0]):
        for j in range(anomaly_data.shape[1]):
            for k in range(anomaly_data.shape[2]):
                for p in range(anomaly_data.shape[3]):
                    if anomaly_data[i, j, k, p] == detected_data[i, j, k, p] == 0:
                        TN += 1
                    elif anomaly_data[i, j, k, p] != 0 and detected_data[i, j, k, p] != 0:
                        TP += 1
                    elif anomaly_data[i, j, k, p] != 0 and detected_data[i, j, k, p] == 0:
                        FN += 1
                    else:
                        FP += 1

    TPR = TP / (TP + FN)
    FPR = FP / (FP + TN)

    return TPR, FPR

    # anomaly_index = np.nonzero(anomaly_data)
    # detect_anomaly_index = np.nonzero(detected_data)
    # normal_index = np.where(anomaly_data == 0)
    # detect_normal_index = np.where(detected_data == 0)


def find_anomaly(original_data, low_rank_data, anomaly_rate):
    # 转化numpy并置于cpu上进行异常检测

    low_rank_data = low_rank_data.cpu()
    low_rank_data = low_rank_data.detach().numpy()
    total_number = low_rank_data.shape[0] * low_rank_data.shape[1] * low_rank_data.shape[2] * low_rank_data.shape[3]
    res_data = original_data - low_rank_data
    # top_k以及索引计算
    top_k = round(total_number * anomaly_rate)
    Top_k_index = k_largest_index_argsort(res_data, k=top_k)
    # 定义异常
    anomaly_data = np.zeros(
        (low_rank_data.shape[0], low_rank_data.shape[1], low_rank_data.shape[2], low_rank_data.shape[3]))
    anomaly_data[Top_k_index[:, 0], Top_k_index[:, 1], Top_k_index[:, 2], Top_k_index[:, 3]] = res_data[
        Top_k_index[:, 0], Top_k_index[:, 1], Top_k_index[:, 2], Top_k_index[:, 3]]
    return anomaly_data


def anomaly_detection(Origin_data, Anomaly_data, max_iter, learn_ratio, anomaly_ratio):
    input_data = Origin_data
    normal_data = input_data
    rank = 5
    TPR = 0
    FPR = 0
    print("begin anomaly detection.")
    t0 = time.time()
    low_rank_data = CNN.low_rank(normal_data, learn_ratio, rank, False)
    for i in range(max_iter):
        detect_data = find_anomaly(input_data, low_rank_data, anomaly_ratio)

        # TPR, FPR = accuracy_detection(Anomaly_data, detect_data)
        # print('TPR FPR:', TPR, FPR)
        normal_data = input_data - detect_data

        low_rank_data = CNN.low_rank(normal_data, learn_ratio, rank, False)
    t1 = time.time()
    total_time = (t1 - t0)
    print("Used_time.", total_time)
    return TPR, FPR


if __name__ == '__main__':
    # 测试代码
    # model = torch.load("model_decompose_fine_tune").cuda()
    # model2 = torch.load("basic_model").cuda()
    # model = torch.load("model_decompose_fine_tune").cuda()

    # data = np.load('../data/AGM0.1C0.1R0.05.npz')
    
    # anomaly_data = data.f.arr_0
    # input_data = torch.Tensor(anomaly_data)
    # #, map_location='cpu'
    # model = torch.load("basic_model").cpu()
    # model.eval()
    # decomposed_model = torch.load("decomposed_model").cpu()
    # decomposed_model.eval()
    # decomposed_model_fine = torch.load("model_decompose_fine_tune").cpu()
    # decomposed_model_fine.eval()
    #
    # t0 = time.time()
    # output1 = model(input_data)
    # t1 = time.time()
    #
    # t2 = time.time()
    # output2 = decomposed_model(input_data)
    # t3 = time.time()
    #
    # t4 = time.time()
    # output = decomposed_model_fine(input_data)
    # t5 = time.time()
    # print("CAE process time", float(t1 - t0), float(t3 - t2), float(t5 - t4))

    #实际运行代码 Abilene数据集
    data = np.load('../data/npzfile/G/C/GGR0.1M0C0.05.npz')
    anomaly_data = data.f.arr_0
    monitoring_data = data.f.arr_1
    normal_data = data.f.arr_2
    outlier_ratio = data.f.arr_3 * 1
    del data
    TPR, FPR = anomaly_detection(monitoring_data, anomaly_data, 20, 0.1, outlier_ratio)





    # 实际运行代码Geant数据集
    # data = np.load('../data/npzfile/G/M/GGR0.1M0.1C1.npz')
    # normal_data = data.f.arr_2
    # anomaly_data = data.f.arr_0
    # monitoring_data = normal_data+anomaly_data
    # outlier_ratio = data.f.arr_3 * 1
    # del data
    # TPR, FPR = anomaly_detection(monitoring_data, anomaly_data, 20, 0.1, outlier_ratio)
